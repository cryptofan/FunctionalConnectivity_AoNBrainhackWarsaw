{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "from scipy import special, stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fc_methods as fc\n",
    "import helpers as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give the source to all datasets:\n",
    "foldername_sample_fMRI = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/codes/'\n",
    "foldername_data_fMRI = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/datasets/fMRI/'\n",
    "foldername_data_MEG = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/datasets/MEG/'\n",
    "foldername_data_google_trends = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/datasets/google_trends/'\n",
    "foldername_data_stock_exchange = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/datasets/stock_exchange/'\n",
    "foldername_data_weather = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/datasets/weather/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give the output folder addresses:\n",
    "output_fMRI = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/fMRI/'\n",
    "output_MEG = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/MEG/'\n",
    "output_google_trends = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/google_trends/'\n",
    "output_stock_exchange = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/stock_exchange/'\n",
    "output_weather = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/weather/'\n",
    "output_Berkson = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/Berkson/'\n",
    "output_synthetic = '/Users/nataliabielczyk/Desktop/variousprojects/AoN_Brainhack/PIPELINE/FunctionalConnectivity_AoNBrainhackWarsaw-master/results/synthetic/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example comparison of correlation methods on random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us compare Pearson and partial correlation on randomly generated data sampled from a uniform distribution.\n",
    "\n",
    "We will start by creating the data matrix with 50 features and 300 \"timepoints\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_matrix = np.random.rand(300,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the correlation and p-values matrices using both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_random, pearson_random_p = fc.pearson_corr(rand_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_random, partial_random_p = fc.partial_corr(rand_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_partial_random, partial_partial_random_p = fc.partial_partial_corr(rand_matrix,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare two implementatiosn of partial correlation on random data:\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "sns.heatmap(partial_random, vmin=-0.15, vmax=0.15, cmap='coolwarm');\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "sns.heatmap(partial_partial_random, vmin=-0.15, vmax=0.15, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualise them side by side, so the differences will be easier to spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot Pearson versus partial correlation, together with the associated statistics p:\n",
    "def plot_corr(pearson, pearson_p, partial_partial, partial_partial_p, partial, partial_p):\n",
    "    fig = plt.figure(figsize=(25,11))\n",
    "    #fig.suptitle('sample fMRI subject, correlations', y=1.05, fontsize=20);\n",
    "    \n",
    "    ax  = plt.subplot2grid((3,6),(0,0), colspan=2, rowspan=2)\n",
    "    ax1 = plt.subplot2grid((3,6),(0,2), colspan=2, rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3,6),(0,4), colspan=2, rowspan=2)\n",
    "    \n",
    "    sns.heatmap(pearson, vmin=-0.5, vmax=0.5, cmap='coolwarm', ax=ax);\n",
    "    ax.set_xlabel('Pearson correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(partial_partial, vmin=-0.5, vmax=0.5, cmap='coolwarm', ax=ax1);\n",
    "    ax1.set_xlabel('Partial partial correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(partial, vmin=-0.5, vmax=0.5, cmap='coolwarm', ax=ax2);\n",
    "    ax2.set_xlabel('Partial correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "\n",
    "    ax3 = plt.subplot2grid((3,6),(2,0))\n",
    "    ax4 = plt.subplot2grid((3,6),(2,1))\n",
    "    \n",
    "    if type(pearson) == type(pd.DataFrame()):\n",
    "        sns.distplot(pearson.values.flatten()[np.where(pearson.values.flatten()<1)], kde=False, ax=ax3);\n",
    "        sns.distplot(pearson_p.values.flatten(), kde=False, ax=ax4);\n",
    "    else:\n",
    "        sns.distplot(pearson.flatten()[np.where(pearson.flatten()<1)], kde=False, ax=ax3);\n",
    "        sns.distplot(pearson_p.flatten(), kde=False, ax=ax4);\n",
    "    ax3.set_title('Distribution of Pearson correlation values', y=1.10);\n",
    "    ax4.set_title('P-values distribution', y=1.10);\n",
    "\n",
    "    ax5 = plt.subplot2grid((3,6),(2,2))\n",
    "    ax6 = plt.subplot2grid((3,6),(2,3))\n",
    "    if type(partial_partial) == type(pd.DataFrame()):\n",
    "        sns.distplot(partial_partial.values.flatten()[np.where(partial_partial.values.flatten()<1)], kde=False, ax=ax5);\n",
    "        sns.distplot(partial_partial_p.values.flatten(), kde=False, ax=ax6);\n",
    "    else:\n",
    "        sns.distplot(partial_partial.flatten()[np.where(partial_partial.flatten()<1)], kde=False, ax=ax5);\n",
    "        sns.distplot(partial_partial_p.flatten(), kde=False, ax=ax6);\n",
    "    ax5.set_title('Distribution of partial partial correlation values', y=1.10);\n",
    "    ax6.set_title('P-values distribution', y=1.10);\n",
    "\n",
    "    ax7 = plt.subplot2grid((3,6),(2,4))\n",
    "    ax8 = plt.subplot2grid((3,6),(2,5))\n",
    "    if type(partial) == type(pd.DataFrame()):\n",
    "        sns.distplot(partial.values.flatten()[np.where(partial.values.flatten()<1)], kde=False, ax=ax7);\n",
    "        sns.distplot(partial_p.values.flatten(), kde=False, ax=ax8);\n",
    "    else:\n",
    "        sns.distplot(partial.flatten()[np.where(partial.flatten()<1)], kde=False, ax=ax7);\n",
    "        sns.distplot(partial_p.flatten(), kde=False, ax=ax8);\n",
    "    ax7.set_title('Distribution of partial correlation values', y=1.10);\n",
    "    ax8.set_title('P-values distribution', y=1.10);\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "        plt.sca(ax)\n",
    "        ax.xaxis.tick_top()\n",
    "        plt.xticks(rotation=90)       \n",
    "                  \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_all(pearson, partial_partial, partial, mi, inv_ed, inv_md, max_pearson, max_partial_partial, max_partial, max_mi, max_inv_ed, max_inv_md):\n",
    "    \n",
    "    fig = plt.figure(figsize=(21,12))\n",
    "    #fig.suptitle('sample fMRI subject', y=1.25, fontsize=20);\n",
    "    \n",
    "    ax  = plt.subplot2grid((2,3),(0,0), colspan=1, rowspan=1)\n",
    "    ax1 = plt.subplot2grid((2,3),(0,1), colspan=1, rowspan=1)\n",
    "    ax2 = plt.subplot2grid((2,3),(0,2), colspan=1, rowspan=1)\n",
    "    ax3 = plt.subplot2grid((2,3),(1,0), colspan=1, rowspan=1)\n",
    "    ax4 = plt.subplot2grid((2,3),(1,1), colspan=1, rowspan=1)\n",
    "    ax5 = plt.subplot2grid((2,3),(1,2), colspan=1, rowspan=1)\n",
    "    \n",
    "    sns.heatmap(pearson, vmin=-abs(max_pearson), vmax=abs(max_pearson), cmap='coolwarm', ax=ax);\n",
    "    ax.set_xlabel('Pearson correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(partial_partial, vmin=-abs(max_partial_partial), vmax=abs(max_partial_partial), cmap='coolwarm', ax=ax1);\n",
    "    ax1.set_xlabel('Partial partial correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(partial, vmin=-abs(max_partial), vmax=abs(max_partial), cmap='coolwarm', ax=ax2);\n",
    "    ax2.set_xlabel('Partial correlation', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(mi, vmin=-abs(max_mi), vmax=abs(max_mi), cmap='coolwarm', ax=ax3);\n",
    "    ax3.set_xlabel('Mutual information', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(inv_ed, vmin=-abs(max_inv_ed), vmax=abs(max_inv_ed), cmap='coolwarm', ax=ax4);\n",
    "    ax4.set_xlabel('Inverse Euclidean distance', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    sns.heatmap(inv_md, vmin=-abs(max_inv_md), vmax=abs(max_inv_md), cmap='coolwarm', ax=ax5);\n",
    "    ax5.set_xlabel('Inverse Manhattan distance', fontsize=16);#, y=1.10);\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "        plt.sca(ax)\n",
    "        ax.xaxis.tick_top()\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "            \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one could expect, when dealing with random data, there is practically no difference between Pearson and partial correlation.  \n",
    "Each variable is fairly independent from others, so partial correlation gives the same results (no additional correlations that we have to control for)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example comparison of correlation methods on generated correlated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will see how these methods will work with correlated data.  \n",
    "We will generate some synthetic data which will follow a predefined correlation structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_size         = 10\n",
    "network_density      = 0.25\n",
    "conn_weight          = 0.15\n",
    "\n",
    "noise_level          = 0.1   \n",
    "T                    = 1000  \n",
    "dt                   = 0.01  \n",
    "    \n",
    "true_fc = fc.generate_fc(network_size, network_density, conn_weight) \n",
    "data_synthetic = fc.simulate_data(true_fc, noise_level, T, dt)\n",
    "n = data_synthetic.shape[0]\n",
    "m = data_synthetic.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print true connectivity:\n",
    "x = np.arange(network_size)+0.5\n",
    "labels = np.ndarray.tolist(np.arange(network_size)+1)\n",
    "\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "sns.heatmap(true_fc, vmin=0, vmax=0.1, cmap='coolwarm');\n",
    "ax = plt.axes()\n",
    "ax.set_title('True underlying functional connectivity')\n",
    "plt.xticks(x, labels)\n",
    "plt.yticks(x, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "folder_output = output_synthetic\n",
    "fname = folder_output + 'true_fc'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_synthetic, pearson_synthetic_p = (pd.DataFrame(fc.pearson_corr(data_synthetic)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_synthetic)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_synthetic = np.max(fc.pearson_corr(data_synthetic)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_synthetic, partial_partial_synthetic_p = (pd.DataFrame(fc.partial_partial_corr(data_synthetic,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_synthetic,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_synthetic = np.max(fc.partial_partial_corr(data_synthetic,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_synthetic, partial_synthetic_p = (pd.DataFrame(fc.partial_partial_corr(data_synthetic,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_synthetic,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_synthetic = np.max(fc.partial_partial_corr(data_synthetic,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_synthetic = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_synthetic[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_synthetic[i,j] = fc.calc_MI(data_synthetic[:,i], data_synthetic[:,j], nbins)\n",
    "        mi_synthetic[j,i] = mi_synthetic[i,j]\n",
    "\n",
    "max_mi_synthetic = np.max(mi_synthetic[np.triu_indices(m,1)])\n",
    "mi_synthetic = pd.DataFrame(mi_synthetic, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_synthetic = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_synthetic[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_synthetic[i,j] = 1.0/fc.euclidean_distance(data_synthetic[:,i], data_synthetic[:,j])\n",
    "        inv_ed_synthetic[j,i] = inv_ed_synthetic[i,j]\n",
    "        \n",
    "max_inv_ed_synthetic = np.max(inv_ed_synthetic[np.triu_indices(m,1)])\n",
    "inv_ed_synthetic = pd.DataFrame(inv_ed_synthetic, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_synthetic = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_synthetic[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_synthetic[i,j] = 1.0/fc.manhattan_distance(data_synthetic[:,i], data_synthetic[:,j])\n",
    "        inv_md_synthetic[j,i] = inv_md_synthetic[i,j]\n",
    "        \n",
    "max_inv_md_synthetic = np.max(inv_md_synthetic[np.triu_indices(m,1)])\n",
    "inv_md_synthetic = pd.DataFrame(inv_md_synthetic, index=labels, columns=labels)\n",
    "\n",
    "corr_synthetic = plot_corr(pearson_synthetic, pearson_synthetic_p, partial_partial_synthetic, partial_partial_synthetic_p, partial_synthetic, partial_synthetic_p)\n",
    "\n",
    "all_synthetic = plot_all(pearson_synthetic, partial_partial_synthetic, partial_synthetic, mi_synthetic, inv_ed_synthetic, inv_md_synthetic, max_pearson_synthetic, max_partial_partial_synthetic, max_partial_synthetic, max_mi_synthetic, max_inv_ed_synthetic, max_inv_md_synthetic)\n",
    "\n",
    "folder_output = output_synthetic\n",
    "fname = folder_output + 'all_methods_synthetic'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_synthetic, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_synthetic\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_synthetic, vmin=-abs(max_pearson_synthetic), vmax=abs(max_pearson_synthetic), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_synthetic, vmin=-abs(max_partial_partial_synthetic), vmax=abs(max_partial_partial_synthetic), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_synthetic, vmin=-abs(max_partial_synthetic), vmax=abs(max_partial_synthetic), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_synthetic, vmin=-abs(max_mi_synthetic), vmax=abs(max_mi_synthetic), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_synthetic, vmin=-abs(max_inv_ed_synthetic), vmax=abs(max_inv_ed_synthetic), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_synthetic, vmin=-abs(max_inv_md_synthetic), vmax=abs(max_inv_md_synthetic), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_synthetic_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "        \n",
    "# add detrending:\n",
    "data_synthetic_detrended = fc.detrend(data_synthetic)\n",
    "\n",
    "# Pearson correlation:\n",
    "pearson_synthetic_detrended, pearson_synthetic_p_detrended = (pd.DataFrame(fc.pearson_corr(data_synthetic_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_synthetic_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_synthetic_detrended = np.max(fc.pearson_corr(data_synthetic_detrended)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_synthetic_detrended, partial_partial_synthetic_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_synthetic_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_synthetic_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_synthetic_detrended = np.max(fc.partial_partial_corr(data_synthetic_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_synthetic_detrended, partial_synthetic_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_synthetic_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_synthetic_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_synthetic_detrended = np.max(fc.partial_partial_corr(data_synthetic_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_synthetic_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_synthetic_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_synthetic_detrended[i,j] = fc.calc_MI(data_synthetic_detrended[:,i], data_synthetic_detrended[:,j], nbins)\n",
    "        mi_synthetic_detrended[j,i] = mi_synthetic_detrended[i,j]\n",
    "\n",
    "max_mi_synthetic_detrended = np.max(mi_synthetic_detrended[np.triu_indices(m,1)])\n",
    "mi_synthetic_detrended = pd.DataFrame(mi_synthetic_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_synthetic_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_synthetic_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_synthetic_detrended[i,j] = 1.0/fc.euclidean_distance(data_synthetic_detrended[:,i], data_synthetic_detrended[:,j])\n",
    "        inv_ed_synthetic_detrended[j,i] = inv_ed_synthetic_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_synthetic_detrended = np.max(inv_ed_synthetic_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_synthetic_detrended = pd.DataFrame(inv_ed_synthetic_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_synthetic_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_synthetic_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_synthetic_detrended[i,j] = 1.0/fc.manhattan_distance(data_synthetic_detrended[:,i], data_synthetic_detrended[:,j])\n",
    "        inv_md_synthetic_detrended[j,i] = inv_md_synthetic_detrended[i,j]\n",
    "        \n",
    "max_inv_md_synthetic_detrended = np.max(inv_md_synthetic_detrended[np.triu_indices(m,1)])\n",
    "inv_md_synthetic_detrended = pd.DataFrame(inv_md_synthetic_detrended, index=labels, columns=labels)\n",
    "\n",
    "corr_synthetic_detrended = plot_corr(pearson_synthetic_detrended, pearson_synthetic_p_detrended, partial_partial_synthetic_detrended, partial_partial_synthetic_p_detrended, partial_synthetic_detrended, partial_synthetic_p_detrended)\n",
    "\n",
    "all_synthetic_detrended = plot_all(pearson_synthetic_detrended, partial_partial_synthetic_detrended, partial_synthetic_detrended, mi_synthetic_detrended, inv_ed_synthetic_detrended, inv_md_synthetic_detrended, max_pearson_synthetic_detrended, max_partial_partial_synthetic_detrended, max_partial_synthetic_detrended, max_mi_synthetic_detrended, max_inv_ed_synthetic_detrended, max_inv_md_synthetic_detrended)\n",
    "\n",
    "folder_output = output_synthetic\n",
    "fname = folder_output + 'all_methods_synthetic_detrended'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_synthetic_detrended, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_synthetic\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_synthetic_detrended, vmin=-abs(max_pearson_synthetic_detrended), vmax=abs(max_pearson_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_synthetic_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_synthetic_detrended, vmin=-abs(max_partial_partial_synthetic_detrended), vmax=abs(max_partial_partial_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_synthetic_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_synthetic_detrended, vmin=-abs(max_partial_synthetic_detrended), vmax=abs(max_partial_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_synthetic_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_synthetic_detrended, vmin=-abs(max_mi_synthetic_detrended), vmax=abs(max_mi_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_synthetic_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_synthetic_detrended, vmin=-abs(max_inv_ed_synthetic_detrended), vmax=abs(max_inv_ed_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_synthetic_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_synthetic_detrended, vmin=-abs(max_inv_md_synthetic_detrended), vmax=abs(max_inv_md_synthetic_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_synthetic_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_synthetic_vectorized = np.ndarray.reshape(np.asarray(pearson_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_synthetic_vectorized = np.ndarray.reshape(np.asarray(partial_partial_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_synthetic_vectorized = np.ndarray.reshape(np.asarray(partial_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_synthetic_vectorized = np.ndarray.reshape(np.asarray(mi_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_synthetic_vectorized = np.ndarray.reshape(np.asarray(inv_ed_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_synthetic_vectorized = np.ndarray.reshape(np.asarray(inv_md_synthetic)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_synthetic_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_synthetic_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "results_allmethods = np.concatenate((pearson_synthetic_vectorized, partial_partial_synthetic_vectorized, partial_synthetic_vectorized, \\\n",
    "                                     mi_synthetic_vectorized, inv_ed_synthetic_vectorized, inv_md_synthetic_vectorized, \\\n",
    "                                     pearson_synthetic_detrended_vectorized, partial_partial_synthetic_detrended_vectorized, partial_synthetic_detrended_vectorized, \\\n",
    "                                     mi_synthetic_detrended_vectorized, inv_ed_synthetic_detrended_vectorized, inv_md_synthetic_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "folder_output = output_synthetic\n",
    "fname = folder_output + 'correlations_all_methods_synthetic'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the correlation and p-values matrices using both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualise them side by side, so the differences will be easier to spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_sim = plot_corr_synthetic(true_fc, pearson_sim, pearson_sim_p, partial_sim, partial_sim_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = '../results/1_Pearson_vs_Partial/'\n",
    "fname = folder_output + 'corr_sim'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(corr_sim, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.clustermap(pearson_sim, vmin=-0.5, vmax=0.5, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on fMRI resting-state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_label = '100307'\n",
    "path = foldername_data_fMRI + 'HCP_parcellated_smoothing5_100subjects_Glasser_restingstate_day1/' + subject_label + '.csv'\n",
    "label_path = foldername_data_fMRI + 'labels_Glasser.csv'\n",
    "labels_all = pd.read_csv(label_path, header=None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose areas within:\n",
    "# 1 - Primary Visual Cortex V1\n",
    "# 2 - Early Visual Cortex\n",
    "# 3 - Dorsal Stream Visual Cortex\n",
    "# 4 - Ventral Stream Visual Cortex\n",
    "# 5 - MT+ Complex and Neighboring Visual Areas\n",
    "indices_visual0 = [1,2,3,4,5,6,7,11,13,16,17,18,19,20,21,22,23,50,119,121,126,127,133,135,136,137,138,140,141,142,143,146,152,153,154,155,156,157,158,159,160,163]\n",
    "indices_visual = [x - 1 for x in indices_visual0]\n",
    "\n",
    "# 6 - Somatosensory and Motor Cortex\n",
    "indices_somatosensory0 = [8,9,10,11,12,36,39,40,42,47,51,52,53,54,55,56,99,100,116,117,147]\n",
    "indices_somatosensory = [x - 1 for x in indices_somatosensory0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load fMRI data:\n",
    "data_fmri0 = np.asarray(pd.read_csv(path, header=None))\n",
    "n = data_fmri0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [a] choose visual system:\n",
    "data_fmri = data_fmri0[:,indices_visual]\n",
    "m = data_fmri.shape[1]\n",
    "labels = labels_all[indices_visual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_fmri, pearson_fmri_p = (pd.DataFrame(fc.pearson_corr(data_fmri)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_fmri)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_fmri = np.max(fc.pearson_corr(data_fmri)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_fmri, partial_partial_fmri_p = (pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_fmri = np.max(fc.partial_partial_corr(data_fmri,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_fmri, partial_fmri_p = (pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_fmri = np.max(fc.partial_partial_corr(data_fmri,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_fmri[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_fmri[i,j] = fc.calc_MI(data_fmri[:,i], data_fmri[:,j], nbins)\n",
    "        mi_fmri[j,i] = mi_fmri[i,j]\n",
    "\n",
    "max_mi_fmri = np.max(mi_fmri[np.triu_indices(m,1)])\n",
    "mi_fmri = pd.DataFrame(mi_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_fmri[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_fmri[i,j] = 1.0/fc.euclidean_distance(data_fmri[:,i], data_fmri[:,j])\n",
    "        inv_ed_fmri[j,i] = inv_ed_fmri[i,j]\n",
    "        \n",
    "max_inv_ed_fmri = np.max(inv_ed_fmri[np.triu_indices(m,1)])\n",
    "inv_ed_fmri = pd.DataFrame(inv_ed_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_fmri[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_fmri[i,j] = 1.0/fc.manhattan_distance(data_fmri[:,i], data_fmri[:,j])\n",
    "        inv_md_fmri[j,i] = inv_md_fmri[i,j]\n",
    "        \n",
    "max_inv_md_fmri = np.max(inv_md_fmri[np.triu_indices(m,1)])\n",
    "inv_md_fmri = pd.DataFrame(inv_md_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_fmri = plot_corr(pearson_fmri, pearson_fmri_p, partial_partial_fmri, partial_partial_fmri_p, partial_fmri, partial_fmri_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_fmri = plot_all(pearson_fmri, partial_partial_fmri, partial_fmri, mi_fmri, inv_ed_fmri, inv_md_fmri, max_pearson_fmri, max_partial_partial_fmri, max_partial_fmri, max_mi_fmri, max_inv_ed_fmri, max_inv_md_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'all_methods_fmri_visual_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_fmri, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_fMRI\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_fmri, vmin=-abs(max_pearson_fmri), vmax=abs(max_pearson_fmri), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_fmri, vmin=-abs(max_partial_partial_fmri), vmax=abs(max_partial_partial_fmri), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_fmri, vmin=-abs(max_partial_fmri), vmax=abs(max_partial_fmri), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_fmri, vmin=-abs(max_mi_fmri), vmax=abs(max_mi_fmri), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_fmri, vmin=-abs(max_inv_ed_fmri), vmax=abs(max_inv_ed_fmri), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_fmri, vmin=-abs(max_inv_md_fmri), vmax=abs(max_inv_md_fmri), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_fmri_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add detrending:\n",
    "data_fmri_detrended = fc.detrend(data_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_fmri_detrended, pearson_fmri_p_detrended = (pd.DataFrame(fc.pearson_corr(data_fmri_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_fmri_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_fmri_detrended = np.max(fc.pearson_corr(data_fmri_detrended)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_fmri_detrended, partial_partial_fmri_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_fmri_detrended = np.max(fc.partial_partial_corr(data_fmri_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_fmri_detrended, partial_fmri_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_fmri_detrended = np.max(fc.partial_partial_corr(data_fmri_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_fmri_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_fmri_detrended[i,j] = fc.calc_MI(data_fmri_detrended[:,i], data_fmri_detrended[:,j], nbins)\n",
    "        mi_fmri_detrended[j,i] = mi_fmri_detrended[i,j]\n",
    "\n",
    "max_mi_fmri_detrended = np.max(mi_fmri_detrended[np.triu_indices(m,1)])\n",
    "mi_fmri_detrended = pd.DataFrame(mi_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_fmri_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_fmri_detrended[i,j] = 1.0/fc.euclidean_distance(data_fmri_detrended[:,i], data_fmri_detrended[:,j])\n",
    "        inv_ed_fmri_detrended[j,i] = inv_ed_fmri_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_fmri_detrended = np.max(inv_ed_fmri_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_fmri_detrended = pd.DataFrame(inv_ed_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_fmri_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_fmri_detrended[i,j] = 1.0/fc.manhattan_distance(data_fmri_detrended[:,i], data_fmri_detrended[:,j])\n",
    "        inv_md_fmri_detrended[j,i] = inv_md_fmri_detrended[i,j]\n",
    "        \n",
    "max_inv_md_fmri_detrended = np.max(inv_md_fmri_detrended[np.triu_indices(m,1)])\n",
    "inv_md_fmri_detrended = pd.DataFrame(inv_md_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_fmri_detrended = plot_corr(pearson_fmri_detrended, pearson_fmri_p_detrended, partial_partial_fmri_detrended, partial_partial_fmri_p_detrended, partial_fmri_detrended, partial_fmri_p_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_fmri_detrended = plot_all(pearson_fmri_detrended, partial_partial_fmri_detrended, partial_fmri_detrended, mi_fmri_detrended, inv_ed_fmri_detrended, inv_md_fmri_detrended, max_pearson_fmri_detrended, max_partial_partial_fmri_detrended, max_partial_fmri_detrended, max_mi_fmri_detrended, max_inv_ed_fmri_detrended, max_inv_md_fmri_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'all_methods_fmri_detrended_visual_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_fmri_detrended, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_fMRI\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_fmri_detrended, vmin=-abs(max_pearson_fmri_detrended), vmax=abs(max_pearson_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_fmri_detrended, vmin=-abs(max_partial_partial_fmri_detrended), vmax=abs(max_partial_partial_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_fmri_detrended, vmin=-abs(max_partial_fmri_detrended), vmax=abs(max_partial_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_fmri_detrended, vmin=-abs(max_mi_fmri_detrended), vmax=abs(max_mi_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_fmri_detrended, vmin=-abs(max_inv_ed_fmri_detrended), vmax=abs(max_inv_ed_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_fmri_detrended, vmin=-abs(max_inv_md_fmri_detrended), vmax=abs(max_inv_md_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_fmri_detrended_visual_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_fmri_vectorized = np.ndarray.reshape(np.asarray(pearson_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_fmri_vectorized = np.ndarray.reshape(np.asarray(partial_partial_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_fmri_vectorized = np.ndarray.reshape(np.asarray(partial_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_fmri_vectorized = np.ndarray.reshape(np.asarray(mi_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_fmri_vectorized = np.ndarray.reshape(np.asarray(inv_ed_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_fmri_vectorized = np.ndarray.reshape(np.asarray(inv_md_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_fmri_detrended)[np.triu_indices(m,1)], nl, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_allmethods = np.concatenate((pearson_fmri_vectorized, partial_partial_fmri_vectorized, partial_fmri_vectorized, \\\n",
    "                                     mi_fmri_vectorized, inv_ed_fmri_vectorized, inv_md_fmri_vectorized, \\\n",
    "                                     pearson_fmri_detrended_vectorized, partial_partial_fmri_detrended_vectorized, partial_fmri_detrended_vectorized, \\\n",
    "                                     mi_fmri_detrended_vectorized, inv_ed_fmri_detrended_vectorized, inv_md_fmri_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'correlations_all_methods_fmri_visual_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [b] choose somatosensory system:\n",
    "data_fmri = data_fmri0[:,indices_somatosensory]\n",
    "m = data_fmri.shape[1]\n",
    "labels = labels_all[indices_somatosensory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_fmri, pearson_fmri_p = (pd.DataFrame(fc.pearson_corr(data_fmri)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_fmri)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_fmri = np.max(fc.pearson_corr(data_fmri)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_fmri, partial_partial_fmri_p = (pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_fmri = np.max(fc.partial_partial_corr(data_fmri,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_fmri, partial_fmri_p = (pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_fmri = np.max(fc.partial_partial_corr(data_fmri,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_fmri[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_fmri[i,j] = fc.calc_MI(data_fmri[:,i], data_fmri[:,j], nbins)\n",
    "        mi_fmri[j,i] = mi_fmri[i,j]\n",
    "\n",
    "max_mi_fmri = np.max(mi_fmri[np.triu_indices(m,1)])\n",
    "mi_fmri = pd.DataFrame(mi_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_fmri[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_fmri[i,j] = 1.0/fc.euclidean_distance(data_fmri[:,i], data_fmri[:,j])\n",
    "        inv_ed_fmri[j,i] = inv_ed_fmri[i,j]\n",
    "        \n",
    "max_inv_ed_fmri = np.max(inv_ed_fmri[np.triu_indices(m,1)])\n",
    "inv_ed_fmri = pd.DataFrame(inv_ed_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_fmri = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_fmri[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_fmri[i,j] = 1.0/fc.manhattan_distance(data_fmri[:,i], data_fmri[:,j])\n",
    "        inv_md_fmri[j,i] = inv_md_fmri[i,j]\n",
    "        \n",
    "max_inv_md_fmri = np.max(inv_md_fmri[np.triu_indices(m,1)])\n",
    "inv_md_fmri = pd.DataFrame(inv_md_fmri, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_fmri = plot_corr(pearson_fmri, pearson_fmri_p, partial_partial_fmri, partial_partial_fmri_p, partial_fmri, partial_fmri_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_fmri = plot_all(pearson_fmri, partial_partial_fmri, partial_fmri, mi_fmri, inv_ed_fmri, inv_md_fmri, max_pearson_fmri, max_partial_partial_fmri, max_partial_fmri, max_mi_fmri, max_inv_ed_fmri, max_inv_md_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'all_methods_fmri_somatosensory_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_fmri, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_fMRI\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_fmri, vmin=-abs(max_pearson_fmri), vmax=abs(max_pearson_fmri), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_fmri, vmin=-abs(max_partial_partial_fmri), vmax=abs(max_partial_partial_fmri), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_fmri, vmin=-abs(max_partial_fmri), vmax=abs(max_partial_fmri), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_fmri, vmin=-abs(max_mi_fmri), vmax=abs(max_mi_fmri), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_fmri, vmin=-abs(max_inv_ed_fmri), vmax=abs(max_inv_ed_fmri), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_fmri, vmin=-abs(max_inv_md_fmri), vmax=abs(max_inv_md_fmri), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_fmri_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add detrending:\n",
    "data_fmri_detrended = fc.detrend(data_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_fmri_detrended, pearson_fmri_p_detrended = (pd.DataFrame(fc.pearson_corr(data_fmri_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_fmri_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_fmri_detrended = np.max(fc.pearson_corr(data_fmri_detrended)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_fmri_detrended, partial_partial_fmri_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_fmri_detrended = np.max(fc.partial_partial_corr(data_fmri_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_fmri_detrended, partial_fmri_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_fmri_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_fmri_detrended = np.max(fc.partial_partial_corr(data_fmri_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_fmri_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_fmri_detrended[i,j] = fc.calc_MI(data_fmri_detrended[:,i], data_fmri_detrended[:,j], nbins)\n",
    "        mi_fmri_detrended[j,i] = mi_fmri_detrended[i,j]\n",
    "\n",
    "max_mi_fmri_detrended = np.max(mi_fmri_detrended[np.triu_indices(m,1)])\n",
    "mi_fmri_detrended = pd.DataFrame(mi_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_fmri_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_fmri_detrended[i,j] = 1.0/fc.euclidean_distance(data_fmri_detrended[:,i], data_fmri_detrended[:,j])\n",
    "        inv_ed_fmri_detrended[j,i] = inv_ed_fmri_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_fmri_detrended = np.max(inv_ed_fmri_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_fmri_detrended = pd.DataFrame(inv_ed_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_fmri_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_fmri_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_fmri_detrended[i,j] = 1.0/fc.manhattan_distance(data_fmri_detrended[:,i], data_fmri_detrended[:,j])\n",
    "        inv_md_fmri_detrended[j,i] = inv_md_fmri_detrended[i,j]\n",
    "        \n",
    "max_inv_md_fmri_detrended = np.max(inv_md_fmri_detrended[np.triu_indices(m,1)])\n",
    "inv_md_fmri_detrended = pd.DataFrame(inv_md_fmri_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_fmri_detrended = plot_corr(pearson_fmri_detrended, pearson_fmri_p_detrended, partial_partial_fmri_detrended, partial_partial_fmri_p_detrended, partial_fmri_detrended, partial_fmri_p_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_fmri_detrended = plot_all(pearson_fmri_detrended, partial_partial_fmri_detrended, partial_fmri_detrended, mi_fmri_detrended, inv_ed_fmri_detrended, inv_md_fmri_detrended, max_pearson_fmri_detrended, max_partial_partial_fmri_detrended, max_partial_fmri_detrended, max_mi_fmri_detrended, max_inv_ed_fmri_detrended, max_inv_md_fmri_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'all_methods_fmri_detrended_somatosensory_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_fmri_detrended, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_fMRI\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_fmri_detrended, vmin=-abs(max_pearson_fmri_detrended), vmax=abs(max_pearson_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_fmri_detrended, vmin=-abs(max_partial_partial_fmri_detrended), vmax=abs(max_partial_partial_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_fmri_detrended, vmin=-abs(max_partial_fmri_detrended), vmax=abs(max_partial_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_fmri_detrended, vmin=-abs(max_mi_fmri_detrended), vmax=abs(max_mi_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_fmri_detrended, vmin=-abs(max_inv_ed_fmri_detrended), vmax=abs(max_inv_ed_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_fmri_detrended, vmin=-abs(max_inv_md_fmri_detrended), vmax=abs(max_inv_md_fmri_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_fmri_detrended_somatosensory_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_fmri_vectorized = np.ndarray.reshape(np.asarray(pearson_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_fmri_vectorized = np.ndarray.reshape(np.asarray(partial_partial_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_fmri_vectorized = np.ndarray.reshape(np.asarray(partial_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_fmri_vectorized = np.ndarray.reshape(np.asarray(mi_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_fmri_vectorized = np.ndarray.reshape(np.asarray(inv_ed_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_fmri_vectorized = np.ndarray.reshape(np.asarray(inv_md_fmri)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_fmri_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_fmri_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_fmri_detrended)[np.triu_indices(m,1)], nl, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_allmethods = np.concatenate((pearson_fmri_vectorized, partial_partial_fmri_vectorized, partial_fmri_vectorized, \\\n",
    "                                     mi_fmri_vectorized, inv_ed_fmri_vectorized, inv_md_fmri_vectorized, \\\n",
    "                                     pearson_fmri_detrended_vectorized, partial_partial_fmri_detrended_vectorized, partial_fmri_detrended_vectorized, \\\n",
    "                                     mi_fmri_detrended_vectorized, inv_ed_fmri_detrended_vectorized, inv_md_fmri_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_fMRI\n",
    "fname = folder_output + 'correlations_all_methods_fmri_somatosensory_system_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on MEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_label = '100307'\n",
    "path = foldername_data_MEG + subject_label + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_meg = pd.read_csv(path, header=0)\n",
    "labels = data_meg.columns\n",
    "data_meg = np.asarray(data_meg)\n",
    "\n",
    "# Choose a subset of regions to limit the network size (provisoric version):\n",
    "Nreg = 20\n",
    "labels = labels[:Nreg]\n",
    "data_meg = data_meg[:,:Nreg]\n",
    "\n",
    "n = data_meg.shape[0]\n",
    "m = data_meg.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_meg, pearson_meg_p = (pd.DataFrame(fc.pearson_corr(data_meg)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_meg)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_meg = np.max(fc.pearson_corr(data_meg)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_meg, partial_partial_meg_p = (pd.DataFrame(fc.partial_partial_corr(data_meg,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_meg,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_meg = np.max(fc.partial_partial_corr(data_meg,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_meg, partial_meg_p = (pd.DataFrame(fc.partial_partial_corr(data_meg,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_meg,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_meg = np.max(fc.partial_partial_corr(data_meg,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_meg = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_meg[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_meg[i,j] = fc.calc_MI(data_meg[:,i], data_meg[:,j], nbins)\n",
    "        mi_meg[j,i] = mi_meg[i,j]\n",
    "\n",
    "max_mi_meg = np.max(mi_meg[np.triu_indices(m,1)])\n",
    "mi_meg = pd.DataFrame(mi_meg, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_meg = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_meg[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_meg[i,j] = 1.0/fc.euclidean_distance(data_meg[:,i], data_meg[:,j])\n",
    "        inv_ed_meg[j,i] = inv_ed_meg[i,j]\n",
    "        \n",
    "max_inv_ed_meg = np.max(inv_ed_meg[np.triu_indices(m,1)])\n",
    "inv_ed_meg = pd.DataFrame(inv_ed_meg, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_meg = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_meg[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_meg[i,j] = 1.0/fc.manhattan_distance(data_meg[:,i], data_meg[:,j])\n",
    "        inv_md_meg[j,i] = inv_md_meg[i,j]\n",
    "        \n",
    "max_inv_md_meg = np.max(inv_md_meg[np.triu_indices(m,1)])\n",
    "inv_md_meg = pd.DataFrame(inv_md_meg, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_meg = plot_corr(pearson_meg, pearson_meg_p, partial_partial_meg, partial_partial_meg_p, partial_meg, partial_meg_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_meg = plot_all(pearson_meg, partial_partial_meg, partial_meg, mi_meg, inv_ed_meg, inv_md_meg, max_pearson_meg, max_partial_partial_meg, max_partial_meg, max_mi_meg, max_inv_ed_meg, max_inv_md_meg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_MEG\n",
    "fname = folder_output + 'all_methods_meg_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_meg, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_MEG\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_meg, vmin=-abs(max_pearson_meg), vmax=abs(max_pearson_meg), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_meg, vmin=-abs(max_partial_partial_meg), vmax=abs(max_partial_partial_meg), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_meg, vmin=-abs(max_partial_meg), vmax=abs(max_partial_meg), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_meg, vmin=-abs(max_mi_meg), vmax=abs(max_mi_meg), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_meg, vmin=-abs(max_inv_ed_meg), vmax=abs(max_inv_ed_meg), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_meg, vmin=-abs(max_inv_md_meg), vmax=abs(max_inv_md_meg), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_meg_system_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add detrending:\n",
    "data_meg_detrended = fc.detrend(data_meg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_meg_detrended, pearson_meg_p_detrended = (pd.DataFrame(fc.pearson_corr(data_meg_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_meg_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_meg_detrended = np.max(fc.pearson_corr(data_meg_detrended)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_meg_detrended, partial_partial_meg_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_meg_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_meg_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_meg_detrended = np.max(fc.partial_partial_corr(data_meg_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_meg_detrended, partial_meg_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_meg_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_meg_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_meg_detrended = np.max(fc.partial_partial_corr(data_meg_detrended,alpha)[0][np.triu_indices(m,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_meg_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_meg_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_meg_detrended[i,j] = fc.calc_MI(data_meg_detrended[:,i], data_meg_detrended[:,j], nbins)\n",
    "        mi_meg_detrended[j,i] = mi_meg_detrended[i,j]\n",
    "\n",
    "max_mi_meg_detrended = np.max(mi_meg_detrended[np.triu_indices(m,1)])\n",
    "mi_meg_detrended = pd.DataFrame(mi_meg_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Euclidean distance:\n",
    "inv_ed_meg_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_meg_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_meg_detrended[i,j] = 1.0/fc.euclidean_distance(data_meg_detrended[:,i], data_meg_detrended[:,j])\n",
    "        inv_ed_meg_detrended[j,i] = inv_ed_meg_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_meg_detrended = np.max(inv_ed_meg_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_meg_detrended = pd.DataFrame(inv_ed_meg_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse of Manhattan distance:\n",
    "inv_md_meg_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_meg_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_meg_detrended[i,j] = 1.0/fc.manhattan_distance(data_meg_detrended[:,i], data_meg_detrended[:,j])\n",
    "        inv_md_meg_detrended[j,i] = inv_md_meg_detrended[i,j]\n",
    "        \n",
    "max_inv_md_meg_detrended = np.max(inv_md_meg_detrended[np.triu_indices(m,1)])\n",
    "inv_md_meg_detrended = pd.DataFrame(inv_md_meg_detrended, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_meg_detrended = plot_corr(pearson_meg_detrended, pearson_meg_p_detrended, partial_partial_meg_detrended, partial_partial_meg_p_detrended, partial_meg_detrended, partial_meg_p_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_meg_detrended = plot_all(pearson_meg_detrended, partial_partial_meg_detrended, partial_meg_detrended, mi_meg_detrended, inv_ed_meg_detrended, inv_md_meg_detrended, max_pearson_meg_detrended, max_partial_partial_meg_detrended, max_partial_meg_detrended, max_mi_meg_detrended, max_inv_ed_meg_detrended, max_inv_md_meg_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = output_MEG\n",
    "fname = folder_output + 'all_methods_meg_detrended_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_meg_detrended, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot and save all the clustermaps:\n",
    "folder_output = output_MEG\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_meg_detrended, vmin=-abs(max_pearson_meg_detrended), vmax=abs(max_pearson_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_meg_detrended, vmin=-abs(max_partial_partial_meg_detrended), vmax=abs(max_partial_partial_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_meg_detrended, vmin=-abs(max_partial_meg_detrended), vmax=abs(max_partial_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_meg_detrended, vmin=-abs(max_mi_meg_detrended), vmax=abs(max_mi_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_meg_detrended, vmin=-abs(max_inv_ed_meg_detrended), vmax=abs(max_inv_ed_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_meg_detrended, vmin=-abs(max_inv_md_meg_detrended), vmax=abs(max_inv_md_meg_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_meg_detrended_subject' + \\\n",
    "        subject_label + '_' + str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_meg_vectorized = np.ndarray.reshape(np.asarray(pearson_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_meg_vectorized = np.ndarray.reshape(np.asarray(partial_partial_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_meg_vectorized = np.ndarray.reshape(np.asarray(partial_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_meg_vectorized = np.ndarray.reshape(np.asarray(mi_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_meg_vectorized = np.ndarray.reshape(np.asarray(inv_ed_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_meg_vectorized = np.ndarray.reshape(np.asarray(inv_md_meg)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_meg_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_meg_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "results_allmethods = np.concatenate((pearson_meg_vectorized, partial_partial_meg_vectorized, partial_meg_vectorized, \\\n",
    "                                     mi_meg_vectorized, inv_ed_meg_vectorized, inv_md_meg_vectorized, \\\n",
    "                                     pearson_meg_detrended_vectorized, partial_partial_meg_detrended_vectorized, partial_meg_detrended_vectorized, \\\n",
    "                                     mi_meg_detrended_vectorized, inv_ed_meg_detrended_vectorized, inv_md_meg_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Inverse Mutual information', 'Inverse Euclidean distance', 'Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "folder_output = output_MEG\n",
    "fname = folder_output + 'correlations_all_methods_meg_subject' + subject_label\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on Google trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = foldername_data_google_trends + 'googletrends_allphrases_5years.csv'\n",
    "data_google_trends0 = pd.read_csv(path, header=0, sep=';', index_col='Week')\n",
    "labels = pd.Index.tolist(data_google_trends0.columns)\n",
    "data_google_trends0 = np.asarray(data_google_trends0)\n",
    "\n",
    "n = data_google_trends0.shape[0]\n",
    "m = data_google_trends0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the data with boxcox:\n",
    "# add little number to the data (so that all the values are positive):\n",
    "data_google_trends = data_google_trends0 + 1e-6*np.ones(data_google_trends0.shape)\n",
    "\n",
    "data = data_google_trends[:,0]\n",
    "\n",
    "for ind in range(m):\n",
    "    boxcoxed = sci.stats.boxcox(np.ndarray.reshape(data_google_trends[:,ind], n, 1))\n",
    "    data_google_trends[:,ind] = boxcoxed[0].reshape((n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_google_trends, pearson_google_trends_p = (pd.DataFrame(fc.pearson_corr(data_google_trends)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_google_trends)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_google_trends = np.max(fc.pearson_corr(data_google_trends)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_google_trends, partial_partial_google_trends_p = (pd.DataFrame(fc.partial_partial_corr(data_google_trends,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_google_trends,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_google_trends = np.max(fc.partial_partial_corr(data_google_trends,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_google_trends, partial_google_trends_p = (pd.DataFrame(fc.partial_partial_corr(data_google_trends,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_google_trends,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_google_trends = np.max(fc.partial_partial_corr(data_google_trends,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_google_trends = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_google_trends[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_google_trends[i,j] = fc.calc_MI(data_google_trends[:,i], data_google_trends[:,j], nbins)\n",
    "        mi_google_trends[j,i] = mi_google_trends[i,j]\n",
    "\n",
    "max_mi_google_trends = np.max(mi_google_trends[np.triu_indices(m,1)])\n",
    "mi_google_trends = pd.DataFrame(mi_google_trends, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_google_trends = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_google_trends[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_google_trends[i,j] = 1.0/fc.euclidean_distance(data_google_trends[:,i], data_google_trends[:,j])\n",
    "        inv_ed_google_trends[j,i] = inv_ed_google_trends[i,j]\n",
    "        \n",
    "max_inv_ed_google_trends = np.max(inv_ed_google_trends[np.triu_indices(m,1)])\n",
    "inv_ed_google_trends = pd.DataFrame(inv_ed_google_trends, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_google_trends = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_google_trends[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_google_trends[i,j] = 1.0/fc.manhattan_distance(data_google_trends[:,i], data_google_trends[:,j])\n",
    "        inv_md_google_trends[j,i] = inv_md_google_trends[i,j]\n",
    "        \n",
    "max_inv_md_google_trends = np.max(inv_md_google_trends[np.triu_indices(m,1)])\n",
    "inv_md_google_trends = pd.DataFrame(inv_md_google_trends, index=labels, columns=labels)\n",
    "\n",
    "corr_google_trends = plot_corr(pearson_google_trends, pearson_google_trends_p, partial_partial_google_trends, partial_partial_google_trends_p, partial_google_trends, partial_google_trends_p)\n",
    "\n",
    "all_google_trends = plot_all(pearson_google_trends, partial_partial_google_trends, partial_google_trends, mi_google_trends, inv_ed_google_trends, inv_md_google_trends, max_pearson_google_trends, max_partial_partial_google_trends, max_partial_google_trends, max_mi_google_trends, max_inv_ed_google_trends, max_inv_md_google_trends)\n",
    "\n",
    "folder_output = output_google_trends\n",
    "fname = folder_output + 'all_methods_google_trends'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_google_trends, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_google_trends\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_google_trends, vmin=-abs(max_pearson_google_trends), vmax=abs(max_pearson_google_trends), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_google_trends, vmin=-abs(max_partial_partial_google_trends), vmax=abs(max_partial_partial_google_trends), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_google_trends, vmin=-abs(max_partial_google_trends), vmax=abs(max_partial_google_trends), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_google_trends, vmin=-abs(max_mi_google_trends), vmax=abs(max_mi_google_trends), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_google_trends, vmin=-abs(max_inv_ed_google_trends), vmax=abs(max_inv_ed_google_trends), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_google_trends, vmin=-abs(max_inv_md_google_trends), vmax=abs(max_inv_md_google_trends), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_google_trends_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "        \n",
    "# add detrending:\n",
    "data_google_trends_detrended = fc.detrend(data_google_trends)\n",
    "\n",
    "# Pearson correlation:\n",
    "pearson_google_trends_detrended, pearson_google_trends_p_detrended = (pd.DataFrame(fc.pearson_corr(data_google_trends_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_google_trends_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_google_trends_detrended = np.max(fc.pearson_corr(data_google_trends_detrended)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_google_trends_detrended, partial_partial_google_trends_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_google_trends_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_google_trends_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_google_trends_detrended = np.max(fc.partial_partial_corr(data_google_trends_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_google_trends_detrended, partial_google_trends_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_google_trends_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_google_trends_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_google_trends_detrended = np.max(fc.partial_partial_corr(data_google_trends_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_google_trends_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_google_trends_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_google_trends_detrended[i,j] = fc.calc_MI(data_google_trends_detrended[:,i], data_google_trends_detrended[:,j], nbins)\n",
    "        mi_google_trends_detrended[j,i] = mi_google_trends_detrended[i,j]\n",
    "\n",
    "max_mi_google_trends_detrended = np.max(mi_google_trends_detrended[np.triu_indices(m,1)])\n",
    "mi_google_trends_detrended = pd.DataFrame(mi_google_trends_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_google_trends_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_google_trends_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_google_trends_detrended[i,j] = 1.0/fc.euclidean_distance(data_google_trends_detrended[:,i], data_google_trends_detrended[:,j])\n",
    "        inv_ed_google_trends_detrended[j,i] = inv_ed_google_trends_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_google_trends_detrended = np.max(inv_ed_google_trends_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_google_trends_detrended = pd.DataFrame(inv_ed_google_trends_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_google_trends_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_google_trends_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_google_trends_detrended[i,j] = 1.0/fc.manhattan_distance(data_google_trends_detrended[:,i], data_google_trends_detrended[:,j])\n",
    "        inv_md_google_trends_detrended[j,i] = inv_md_google_trends_detrended[i,j]\n",
    "        \n",
    "max_inv_md_google_trends_detrended = np.max(inv_md_google_trends_detrended[np.triu_indices(m,1)])\n",
    "inv_md_google_trends_detrended = pd.DataFrame(inv_md_google_trends_detrended, index=labels, columns=labels)\n",
    "\n",
    "corr_google_trends_detrended = plot_corr(pearson_google_trends_detrended, pearson_google_trends_p_detrended, partial_partial_google_trends_detrended, partial_partial_google_trends_p_detrended, partial_google_trends_detrended, partial_google_trends_p_detrended)\n",
    "\n",
    "all_google_trends_detrended = plot_all(pearson_google_trends_detrended, partial_partial_google_trends_detrended, partial_google_trends_detrended, mi_google_trends_detrended, inv_ed_google_trends_detrended, inv_md_google_trends_detrended, max_pearson_google_trends_detrended, max_partial_partial_google_trends_detrended, max_partial_google_trends_detrended, max_mi_google_trends_detrended, max_inv_ed_google_trends_detrended, max_inv_md_google_trends_detrended)\n",
    "\n",
    "folder_output = output_google_trends\n",
    "fname = folder_output + 'all_methods_google_trends_detrended'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_google_trends_detrended, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_google_trends\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_google_trends_detrended, vmin=-abs(max_pearson_google_trends_detrended), vmax=abs(max_pearson_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_google_trends_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_google_trends_detrended, vmin=-abs(max_partial_partial_google_trends_detrended), vmax=abs(max_partial_partial_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_google_trends_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_google_trends_detrended, vmin=-abs(max_partial_google_trends_detrended), vmax=abs(max_partial_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_google_trends_detrended_' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_google_trends_detrended, vmin=-abs(max_mi_google_trends_detrended), vmax=abs(max_mi_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_google_trends_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_google_trends_detrended, vmin=-abs(max_inv_ed_google_trends_detrended), vmax=abs(max_inv_ed_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_google_trends_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_google_trends_detrended, vmin=-abs(max_inv_md_google_trends_detrended), vmax=abs(max_inv_md_google_trends_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_google_trends_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_google_trends_vectorized = np.ndarray.reshape(np.asarray(pearson_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_google_trends_vectorized = np.ndarray.reshape(np.asarray(partial_partial_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_google_trends_vectorized = np.ndarray.reshape(np.asarray(partial_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_google_trends_vectorized = np.ndarray.reshape(np.asarray(mi_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_google_trends_vectorized = np.ndarray.reshape(np.asarray(inv_ed_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_google_trends_vectorized = np.ndarray.reshape(np.asarray(inv_md_google_trends)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_google_trends_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_google_trends_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "results_allmethods = np.concatenate((pearson_google_trends_vectorized, partial_partial_google_trends_vectorized, partial_google_trends_vectorized, \\\n",
    "                                     mi_google_trends_vectorized, inv_ed_google_trends_vectorized, inv_md_google_trends_vectorized, \\\n",
    "                                     pearson_google_trends_detrended_vectorized, partial_partial_google_trends_detrended_vectorized, partial_google_trends_detrended_vectorized, \\\n",
    "                                     mi_google_trends_detrended_vectorized, inv_ed_google_trends_detrended_vectorized, inv_md_google_trends_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "folder_output = output_google_trends\n",
    "fname = folder_output + 'correlations_all_methods_google_trends'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on stock exchange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose the normalized version of the data:\n",
    "path = foldername_data_stock_exchange + 'detrended-log.csv'\n",
    "# read in only first 1,000 rows as the data is huge:\n",
    "data_stock_exchange = pd.read_csv(path, header=0, index_col='date', nrows=1000)\n",
    "labels = pd.Index.tolist(data_stock_exchange.columns)\n",
    "data_stock_exchange = np.asarray(data_stock_exchange)\n",
    "\n",
    "n = data_stock_exchange.shape[0]\n",
    "m = data_stock_exchange.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_stock_exchange, pearson_stock_exchange_p = (pd.DataFrame(fc.pearson_corr(data_stock_exchange)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_stock_exchange)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_stock_exchange = np.max(fc.pearson_corr(data_stock_exchange)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_stock_exchange, partial_partial_stock_exchange_p = (pd.DataFrame(fc.partial_partial_corr(data_stock_exchange,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_stock_exchange,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_stock_exchange = np.max(fc.partial_partial_corr(data_stock_exchange,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_stock_exchange, partial_stock_exchange_p = (pd.DataFrame(fc.partial_partial_corr(data_stock_exchange,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_stock_exchange,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_stock_exchange = np.max(fc.partial_partial_corr(data_stock_exchange,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_stock_exchange = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_stock_exchange[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_stock_exchange[i,j] = fc.calc_MI(data_stock_exchange[:,i], data_stock_exchange[:,j], nbins)\n",
    "        mi_stock_exchange[j,i] = mi_stock_exchange[i,j]\n",
    "\n",
    "max_mi_stock_exchange = np.max(mi_stock_exchange[np.triu_indices(m,1)])\n",
    "mi_stock_exchange = pd.DataFrame(mi_stock_exchange, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_stock_exchange = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_stock_exchange[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_stock_exchange[i,j] = 1.0/fc.euclidean_distance(data_stock_exchange[:,i], data_stock_exchange[:,j])\n",
    "        inv_ed_stock_exchange[j,i] = inv_ed_stock_exchange[i,j]\n",
    "        \n",
    "max_inv_ed_stock_exchange = np.max(inv_ed_stock_exchange[np.triu_indices(m,1)])\n",
    "inv_ed_stock_exchange = pd.DataFrame(inv_ed_stock_exchange, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_stock_exchange = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_stock_exchange[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_stock_exchange[i,j] = 1.0/fc.manhattan_distance(data_stock_exchange[:,i], data_stock_exchange[:,j])\n",
    "        inv_md_stock_exchange[j,i] = inv_md_stock_exchange[i,j]\n",
    "        \n",
    "max_inv_md_stock_exchange = np.max(inv_md_stock_exchange[np.triu_indices(m,1)])\n",
    "inv_md_stock_exchange = pd.DataFrame(inv_md_stock_exchange, index=labels, columns=labels)\n",
    "\n",
    "corr_stock_exchange = plot_corr(pearson_stock_exchange, pearson_stock_exchange_p, partial_partial_stock_exchange, partial_partial_stock_exchange_p, partial_stock_exchange, partial_stock_exchange_p)\n",
    "\n",
    "all_stock_exchange = plot_all(pearson_stock_exchange, partial_partial_stock_exchange, partial_stock_exchange, mi_stock_exchange, inv_ed_stock_exchange, inv_md_stock_exchange, max_pearson_stock_exchange, max_partial_partial_stock_exchange, max_partial_stock_exchange, max_mi_stock_exchange, max_inv_ed_stock_exchange, max_inv_md_stock_exchange)\n",
    "\n",
    "folder_output = output_stock_exchange\n",
    "fname = folder_output + 'all_methods_stock_exchange'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_stock_exchange, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_stock_exchange\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_stock_exchange, vmin=-abs(max_pearson_stock_exchange), vmax=abs(max_pearson_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_stock_exchange, vmin=-abs(max_partial_partial_stock_exchange), vmax=abs(max_partial_partial_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_stock_exchange, vmin=-abs(max_partial_stock_exchange), vmax=abs(max_partial_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_stock_exchange, vmin=-abs(max_mi_stock_exchange), vmax=abs(max_mi_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_stock_exchange, vmin=-abs(max_inv_ed_stock_exchange), vmax=abs(max_inv_ed_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_stock_exchange, vmin=-abs(max_inv_md_stock_exchange), vmax=abs(max_inv_md_stock_exchange), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_stock_exchange' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# add detrending:\n",
    "data_stock_exchange_detrended = fc.detrend(data_stock_exchange)\n",
    "\n",
    "# Pearson correlation:\n",
    "pearson_stock_exchange_detrended, pearson_stock_exchange_p_detrended = (pd.DataFrame(fc.pearson_corr(data_stock_exchange_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_stock_exchange_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_stock_exchange_detrended = np.max(fc.pearson_corr(data_stock_exchange_detrended)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_stock_exchange_detrended, partial_partial_stock_exchange_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_stock_exchange_detrended = np.max(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_stock_exchange_detrended, partial_stock_exchange_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_stock_exchange_detrended = np.max(fc.partial_partial_corr(data_stock_exchange_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_stock_exchange_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_stock_exchange_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_stock_exchange_detrended[i,j] = fc.calc_MI(data_stock_exchange_detrended[:,i], data_stock_exchange_detrended[:,j], nbins)\n",
    "        mi_stock_exchange_detrended[j,i] = mi_stock_exchange_detrended[i,j]\n",
    "\n",
    "max_mi_stock_exchange_detrended = np.max(mi_stock_exchange_detrended[np.triu_indices(m,1)])\n",
    "mi_stock_exchange_detrended = pd.DataFrame(mi_stock_exchange_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_stock_exchange_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_stock_exchange_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_stock_exchange_detrended[i,j] = 1.0/fc.euclidean_distance(data_stock_exchange_detrended[:,i], data_stock_exchange_detrended[:,j])\n",
    "        inv_ed_stock_exchange_detrended[j,i] = inv_ed_stock_exchange_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_stock_exchange_detrended = np.max(inv_ed_stock_exchange_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_stock_exchange_detrended = pd.DataFrame(inv_ed_stock_exchange_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_stock_exchange_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_stock_exchange_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_stock_exchange_detrended[i,j] = 1.0/fc.manhattan_distance(data_stock_exchange_detrended[:,i], data_stock_exchange_detrended[:,j])\n",
    "        inv_md_stock_exchange_detrended[j,i] = inv_md_stock_exchange_detrended[i,j]\n",
    "        \n",
    "max_inv_md_stock_exchange_detrended = np.max(inv_md_stock_exchange_detrended[np.triu_indices(m,1)])\n",
    "inv_md_stock_exchange_detrended = pd.DataFrame(inv_md_stock_exchange_detrended, index=labels, columns=labels)\n",
    "\n",
    "corr_stock_exchange_detrended = plot_corr(pearson_stock_exchange_detrended, pearson_stock_exchange_p_detrended, partial_partial_stock_exchange_detrended, partial_partial_stock_exchange_p_detrended, partial_stock_exchange_detrended, partial_stock_exchange_p_detrended)\n",
    "\n",
    "all_stock_exchange_detrended = plot_all(pearson_stock_exchange_detrended, partial_partial_stock_exchange_detrended, partial_stock_exchange_detrended, mi_stock_exchange_detrended, inv_ed_stock_exchange_detrended, inv_md_stock_exchange_detrended, max_pearson_stock_exchange_detrended, max_partial_partial_stock_exchange_detrended, max_partial_stock_exchange_detrended, max_mi_stock_exchange_detrended, max_inv_ed_stock_exchange_detrended, max_inv_md_stock_exchange_detrended)\n",
    "\n",
    "folder_output = output_stock_exchange\n",
    "fname = folder_output + 'all_methods_stock_exchange_detrended'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_stock_exchange_detrended, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_stock_exchange\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_stock_exchange_detrended, vmin=-abs(max_pearson_stock_exchange_detrended), vmax=abs(max_pearson_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_stock_exchange_detrended, vmin=-abs(max_partial_partial_stock_exchange_detrended), vmax=abs(max_partial_partial_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_stock_exchange_detrended, vmin=-abs(max_partial_stock_exchange_detrended), vmax=abs(max_partial_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_stock_exchange_detrended, vmin=-abs(max_mi_stock_exchange_detrended), vmax=abs(max_mi_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_stock_exchange_detrended, vmin=-abs(max_inv_ed_stock_exchange_detrended), vmax=abs(max_inv_ed_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_stock_exchange_detrended, vmin=-abs(max_inv_md_stock_exchange_detrended), vmax=abs(max_inv_md_stock_exchange_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_stock_exchange_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(pearson_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(partial_partial_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(partial_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(mi_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(inv_ed_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_stock_exchange_vectorized = np.ndarray.reshape(np.asarray(inv_md_stock_exchange)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_stock_exchange_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_stock_exchange_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "results_allmethods = np.concatenate((pearson_stock_exchange_vectorized, partial_partial_stock_exchange_vectorized, partial_stock_exchange_vectorized, \\\n",
    "                                     mi_stock_exchange_vectorized, inv_ed_stock_exchange_vectorized, inv_md_stock_exchange_vectorized, \\\n",
    "                                     pearson_stock_exchange_detrended_vectorized, partial_partial_stock_exchange_detrended_vectorized, partial_stock_exchange_detrended_vectorized, \\\n",
    "                                     mi_stock_exchange_detrended_vectorized, inv_ed_stock_exchange_detrended_vectorized, inv_md_stock_exchange_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "folder_output = output_stock_exchange\n",
    "fname = folder_output + 'correlations_all_methods_stock_exchange'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose the normalized version of the data:\n",
    "path = foldername_data_weather + 'data_by_airport/5_LosAngeles.csv'\n",
    "data_weather0 = pd.read_csv(path, sep=';', usecols=[2,3,5,8,11,13], index_col='Date').dropna()\n",
    "labels = pd.Index.tolist(data_weather0.columns)\n",
    "data_weather0 = np.asarray(data_weather0)\n",
    "\n",
    "n = data_weather0.shape[0]\n",
    "m = data_weather0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the data with boxcox:\n",
    "# add little number to the data (so that all the values are positive):\n",
    "data_weather = data_weather0 + 1e-6*np.ones(data_weather0.shape)\n",
    "\n",
    "for ind in range(m):\n",
    "    boxcoxed = sci.stats.boxcox(np.ndarray.reshape(data_weather[:,ind], n, 1))\n",
    "    data_weather[:,ind] = boxcoxed[0].reshape((n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pearson correlation:\n",
    "pearson_weather, pearson_weather_p = (pd.DataFrame(fc.pearson_corr(data_weather)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_weather)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_weather = np.max(fc.pearson_corr(data_weather)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_weather, partial_partial_weather_p = (pd.DataFrame(fc.partial_partial_corr(data_weather,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_weather,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_weather = np.max(fc.partial_partial_corr(data_weather,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_weather, partial_weather_p = (pd.DataFrame(fc.partial_partial_corr(data_weather,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_weather,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_weather = np.max(fc.partial_partial_corr(data_weather,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_weather = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_weather[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_weather[i,j] = fc.calc_MI(data_weather[:,i], data_weather[:,j], nbins)\n",
    "        mi_weather[j,i] = mi_weather[i,j]\n",
    "\n",
    "max_mi_weather = np.max(mi_weather[np.triu_indices(m,1)])\n",
    "mi_weather = pd.DataFrame(mi_weather, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_weather = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_weather[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_weather[i,j] = 1.0/fc.euclidean_distance(data_weather[:,i], data_weather[:,j])\n",
    "        inv_ed_weather[j,i] = inv_ed_weather[i,j]\n",
    "        \n",
    "max_inv_ed_weather = np.max(inv_ed_weather[np.triu_indices(m,1)])\n",
    "inv_ed_weather = pd.DataFrame(inv_ed_weather, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_weather = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_weather[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_weather[i,j] = 1.0/fc.manhattan_distance(data_weather[:,i], data_weather[:,j])\n",
    "        inv_md_weather[j,i] = inv_md_weather[i,j]\n",
    "        \n",
    "max_inv_md_weather = np.max(inv_md_weather[np.triu_indices(m,1)])\n",
    "inv_md_weather = pd.DataFrame(inv_md_weather, index=labels, columns=labels)\n",
    "\n",
    "corr_weather = plot_corr(pearson_weather, pearson_weather_p, partial_partial_weather, partial_partial_weather_p, partial_weather, partial_weather_p)\n",
    "\n",
    "all_weather = plot_all(pearson_weather, partial_partial_weather, partial_weather, mi_weather, inv_ed_weather, inv_md_weather, max_pearson_weather, max_partial_partial_weather, max_partial_weather, max_mi_weather, max_inv_ed_weather, max_inv_md_weather)\n",
    "\n",
    "folder_output = output_weather\n",
    "fname = folder_output + 'all_methods_weather'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_weather, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_weather\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_weather, vmin=-abs(max_pearson_weather), vmax=abs(max_pearson_weather), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_weather, vmin=-abs(max_partial_partial_weather), vmax=abs(max_partial_partial_weather), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_weather, vmin=-abs(max_partial_weather), vmax=abs(max_partial_weather), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_weather, vmin=-abs(max_mi_weather), vmax=abs(max_mi_weather), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_weather, vmin=-abs(max_inv_ed_weather), vmax=abs(max_inv_ed_weather), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_weather, vmin=-abs(max_inv_md_weather), vmax=abs(max_inv_md_weather), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_weather' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# add detrending:\n",
    "data_weather_detrended = fc.detrend(data_weather)\n",
    "\n",
    "# Pearson correlation:\n",
    "pearson_weather_detrended, pearson_weather_p_detrended = (pd.DataFrame(fc.pearson_corr(data_weather_detrended)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.pearson_corr(data_weather_detrended)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_pearson_weather_detrended = np.max(fc.pearson_corr(data_weather_detrended)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial partial correlation:\n",
    "alpha = 0.5\n",
    "partial_partial_weather_detrended, partial_partial_weather_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_weather_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_weather_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_partial_weather_detrended = np.max(fc.partial_partial_corr(data_weather_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Partial correlation:\n",
    "alpha = 1.0\n",
    "partial_weather_detrended, partial_weather_p_detrended = (pd.DataFrame(fc.partial_partial_corr(data_weather_detrended,alpha)[0], \n",
    "                                          index=labels, columns=labels),\n",
    "                                pd.DataFrame(fc.partial_partial_corr(data_weather_detrended,alpha)[1], \n",
    "                                          index=labels, columns=labels))\n",
    "max_partial_weather_detrended = np.max(fc.partial_partial_corr(data_weather_detrended,alpha)[0][np.triu_indices(m,1)])\n",
    "\n",
    "# Mutual Information:\n",
    "# take number of bins as square root of the data length:\n",
    "nbins = int(np.floor(np.sqrt(n)))\n",
    "mi_weather_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    mi_weather_detrended[i, i] = 10\n",
    "    for j in range(i+1, m):\n",
    "        mi_weather_detrended[i,j] = fc.calc_MI(data_weather_detrended[:,i], data_weather_detrended[:,j], nbins)\n",
    "        mi_weather_detrended[j,i] = mi_weather_detrended[i,j]\n",
    "\n",
    "max_mi_weather_detrended = np.max(mi_weather_detrended[np.triu_indices(m,1)])\n",
    "mi_weather_detrended = pd.DataFrame(mi_weather_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Euclidean distance:\n",
    "inv_ed_weather_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_ed_weather_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_ed_weather_detrended[i,j] = 1.0/fc.euclidean_distance(data_weather_detrended[:,i], data_weather_detrended[:,j])\n",
    "        inv_ed_weather_detrended[j,i] = inv_ed_weather_detrended[i,j]\n",
    "        \n",
    "max_inv_ed_weather_detrended = np.max(inv_ed_weather_detrended[np.triu_indices(m,1)])\n",
    "inv_ed_weather_detrended = pd.DataFrame(inv_ed_weather_detrended, index=labels, columns=labels)\n",
    "\n",
    "# inverse of Manhattan distance:\n",
    "inv_md_weather_detrended = np.zeros((m,m))\n",
    "for i in range(m):\n",
    "    inv_md_weather_detrended[i, i] = 1.0\n",
    "    for j in range(i+1, m):\n",
    "        inv_md_weather_detrended[i,j] = 1.0/fc.manhattan_distance(data_weather_detrended[:,i], data_weather_detrended[:,j])\n",
    "        inv_md_weather_detrended[j,i] = inv_md_weather_detrended[i,j]\n",
    "        \n",
    "max_inv_md_weather_detrended = np.max(inv_md_weather_detrended[np.triu_indices(m,1)])\n",
    "inv_md_weather_detrended = pd.DataFrame(inv_md_weather_detrended, index=labels, columns=labels)\n",
    "\n",
    "corr_weather_detrended = plot_corr(pearson_weather_detrended, pearson_weather_p_detrended, partial_partial_weather_detrended, partial_partial_weather_p_detrended, partial_weather_detrended, partial_weather_p_detrended)\n",
    "\n",
    "all_weather_detrended = plot_all(pearson_weather_detrended, partial_partial_weather_detrended, partial_weather_detrended, mi_weather_detrended, inv_ed_weather_detrended, inv_md_weather_detrended, max_pearson_weather_detrended, max_partial_partial_weather_detrended, max_partial_weather_detrended, max_mi_weather_detrended, max_inv_ed_weather_detrended, max_inv_md_weather_detrended)\n",
    "\n",
    "folder_output = output_weather\n",
    "fname = folder_output + 'all_methods_weather_detrended'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(all_weather_detrended, fname, dpis, formats)\n",
    "\n",
    "# plot and save all the clustermaps:\n",
    "folder_output = output_weather\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "# Pearson correlation:\n",
    "fig1 = plt.figure(figsize=(12,11))\n",
    "s1 = sns.clustermap(pearson_weather_detrended, vmin=-abs(max_pearson_weather_detrended), vmax=abs(max_pearson_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s1.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s1.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Pearson correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_pearson_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial partial correlation:\n",
    "fig2 = plt.figure(figsize=(12,11))\n",
    "s2 = sns.clustermap(partial_partial_weather_detrended, vmin=-abs(max_partial_partial_weather_detrended), vmax=abs(max_partial_partial_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s2.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s2.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_partial_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Partial correlation: \n",
    "fig3 = plt.figure(figsize=(12,11))\n",
    "s3 = sns.clustermap(partial_weather_detrended, vmin=-abs(max_partial_weather_detrended), vmax=abs(max_partial_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s3.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s3.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Partial correlation', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_partial_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "        \n",
    "# Mutual information: \n",
    "fig4 = plt.figure(figsize=(12,11))\n",
    "s4 = sns.clustermap(mi_weather_detrended, vmin=-abs(max_mi_weather_detrended), vmax=abs(max_mi_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s4.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s4.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Mutual information', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_mi_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Euclidean distance: \n",
    "fig5 = plt.figure(figsize=(12,11))\n",
    "s5 = sns.clustermap(inv_ed_weather_detrended, vmin=-abs(max_inv_ed_weather_detrended), vmax=abs(max_inv_ed_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s5.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s5.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Euclidean distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_ed_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# Manhattan distance: \n",
    "fig6 = plt.figure(figsize=(12,11))\n",
    "s6 = sns.clustermap(inv_md_weather_detrended, vmin=-abs(max_inv_md_weather_detrended), vmax=abs(max_inv_md_weather_detrended), cmap='coolwarm');\n",
    "plt.setp(s6.ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n",
    "plt.setp(s6.ax_heatmap.get_xticklabels(), rotation=90) # For x axis\n",
    "plt.title('Inverse Manhattan distance', fontsize=16, y=1.10);\n",
    "for i in range(len(dpis)):\n",
    "    for j in range(len(formats)):\n",
    "        fname = folder_output + 'clustermap_inv_md_weather_detrended' + \\\n",
    "        str(dpis[i]) + '.' + formats[j]\n",
    "        plt.savefig(fname, dpi=dpis[i], format=formats[j], bbox_inches=\"tight\")\n",
    "\n",
    "# plot correlation between the 12 methods:\n",
    "nl = len(np.triu_indices(m,1)[0])\n",
    "\n",
    "pearson_weather_vectorized = np.ndarray.reshape(np.asarray(pearson_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_weather_vectorized = np.ndarray.reshape(np.asarray(partial_partial_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_weather_vectorized = np.ndarray.reshape(np.asarray(partial_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_weather_vectorized = np.ndarray.reshape(np.asarray(mi_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_weather_vectorized = np.ndarray.reshape(np.asarray(inv_ed_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_weather_vectorized = np.ndarray.reshape(np.asarray(inv_md_weather)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "pearson_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(pearson_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_partial_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_partial_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "partial_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(partial_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "mi_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(mi_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_ed_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_ed_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "inv_md_weather_detrended_vectorized = np.ndarray.reshape(np.asarray(inv_md_weather_detrended)[np.triu_indices(m,1)], nl, 1)\n",
    "\n",
    "results_allmethods = np.concatenate((pearson_weather_vectorized, partial_partial_weather_vectorized, partial_weather_vectorized, \\\n",
    "                                     mi_weather_vectorized, inv_ed_weather_vectorized, inv_md_weather_vectorized, \\\n",
    "                                     pearson_weather_detrended_vectorized, partial_partial_weather_detrended_vectorized, partial_weather_detrended_vectorized, \\\n",
    "                                     mi_weather_detrended_vectorized, inv_ed_weather_detrended_vectorized, inv_md_weather_detrended_vectorized),axis=1)\n",
    "\n",
    "corr_results_allmethods, corr_results_allmethods_p = sci.stats.spearmanr(results_allmethods)\n",
    "labels_allmethods = ['Pearson correlation','Partial partial correlation', 'Partial partial correlation', \\\n",
    "          'Mutual information', 'Inverse Euclidean distance', 'Inverse Manhattan distance', \\\n",
    "          'Pearson correlation, detrended data','Partial partial correlation, detrended data', 'Partial partial correlation, detrended data', \\\n",
    "          'Mutual information, detrended data', 'Inverse Euclidean distance, detrended data', 'Inverse Manhattan distance, detrended data']\n",
    "\n",
    "\n",
    "corr_results_allmethods = pd.DataFrame(corr_results_allmethods, index=labels_allmethods, columns=labels_allmethods)\n",
    "corr_results_allmethods_p = pd.DataFrame(corr_results_allmethods_p, index=labels_allmethods, columns=labels_allmethods)\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax0 = plt.subplot2grid((1,2),(0,0), colspan=1, rowspan=1)\n",
    "ax1 = plt.subplot2grid((1,2),(0,1), colspan=1, rowspan=1)\n",
    "sns.heatmap(corr_results_allmethods, vmin=-1.0, vmax=1.0, cmap='coolwarm', ax=ax0);\n",
    "ax0.set_title('Spearman r between the methods', fontsize=16,y=1.70);\n",
    "sns.heatmap(corr_results_allmethods_p, vmin=0, vmax=0.01, cmap='coolwarm', ax=ax1);\n",
    "ax1.set_title('Associated p values', fontsize=16,  y=1.70);\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    ax.axvline(x=6, color='k', linestyle='-')\n",
    "    ax.axhline(y=6, color='k', linestyle='-')\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.xticks(rotation=90)\n",
    "for label in ax1.get_yticklabels():\n",
    "    label.set_visible(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "folder_output = output_weather\n",
    "fname = folder_output + 'correlations_all_methods_weather'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = + 'data_by_airport/5_LosAngeles.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_weather, pearson_weather_p = fc.pearson_corr(pd.read_csv(path, sep=';', usecols=[2,3,5,8,11,13], index_col='Date').dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_weather, partial_weather_p = fc.partial_corr(pd.read_csv(path, sep=';', usecols=[2,3,5,8,11,13], index_col='Date').dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_weather = plot_corr(pearson_weather, pearson_weather_p, partial_weather, partial_weather_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_output = '../results/1_Pearson_vs_Partial/'\n",
    "fname = folder_output + 'corr_weather'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(corr_weather, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berkson's Paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = np.random.randint(0,1000,100)\n",
    "x3 = np.random.randint(0,1000,100)\n",
    "x2 = 0.5*x1 + 0.5*x3\n",
    "labels = ['1','2','3']\n",
    "x = np.arange(3)+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = np.column_stack([x1,x2,x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,6))\n",
    "sns.heatmap(fc.pearson_corr(xxx)[0], vmin=-1.0, vmax=1.0, cmap='coolwarm');\n",
    "ax = plt.axes()\n",
    "ax.set_title('Pearson correlation')\n",
    "plt.xticks(x, labels)\n",
    "plt.yticks(x, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "folder_output = output_Berkson\n",
    "fname = folder_output + 'Berkson_1'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "sns.heatmap(fc.partial_partial_corr(xxx,alpha)[0], vmin=-0.5, vmax=1, cmap='coolwarm');\n",
    "ax = plt.axes()\n",
    "ax.set_title('Partial partial correlation, alpha = 0.5')\n",
    "plt.xticks(x, labels)\n",
    "plt.yticks(x, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "folder_output = output_Berkson\n",
    "fname = folder_output + 'Berkson_2'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "sns.heatmap(fc.partial_partial_corr(xxx,alpha)[0], vmin=-0.5, vmax=1, cmap='coolwarm');\n",
    "ax = plt.axes()\n",
    "ax.set_title('Partial correlation')\n",
    "plt.xticks(x, labels)\n",
    "plt.yticks(x, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "folder_output = output_Berkson\n",
    "fname = folder_output + 'Berkson_3'\n",
    "\n",
    "dpis = [300, 600]\n",
    "formats = ['jpg', 'png', 'pdf']\n",
    "\n",
    "hp.savef(fig, fname, dpis, formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
